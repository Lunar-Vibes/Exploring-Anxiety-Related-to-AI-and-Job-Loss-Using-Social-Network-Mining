---
title: "Exploring Anxiety Related to AI and Job Loss Using Social Network Mining"
subtitle: "Text Mining & Social Media Mining Project"
author: ""
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
---

<div style="font-size:28px; font-weight:500; margin-top:12px;">
  Shagufta Shaheen (477654)<br>
  Kacper Geisshirt ( 443171)<br>
  Szymon Grabowski (473037)
</div>

<br>


## **1.Introduction**
![](image1.PNG)
The rapid growth of Artificial Intelligence (AI) is changing the way people work. While AI brings new opportunities and improvements in efficiency, it has also created growing concerns about job security, automation, and the risk of losing employment. Many people express these worries online, sharing their thoughts, fears, and personal experiences about how AI may affect their careers and the future of work.

Social media platforms provide an important space where such concerns are openly discussed. Reddit, in particular, hosts many focused communities where users talk in detail about jobs, career uncertainty, and the impact of new technologies. Conversations about AI and job loss often reveal strong emotional reactions, including stress, fear, and anxiety, coming from people across different professions.

This project aims to highlight discussions where worries about AI, job insecurity, and anxiety appear together. By focusing on posts where these themes overlap, the study seeks to better understand how AI-driven changes in the labor market are emotionally experienced within online communities.



## **2. Data Collection**

The data for this project was collected from Reddit using the official Reddit API. OAuth-based authentication was used to obtain an access token, which allowed authorized requests to Reddit‚Äôs endpoints. Posts were collected from the following subreddits:



- `r/jobs`
- `r/cscareerquestions`
- `r/Futurology`
- `r/antiwork`
- `r/singularity`
- `r/technology`
- `r/ArtistHate`
- `r/DataScience`

 For each subreddit, recent posts were retrieved using the *‚Äúnew‚Äù* listing to capture the latest discussions.

All collected posts were combined into a single dataset, and duplicate posts were removed using unique post identifiers. To reduce noise and keep the dataset aligned with the project focus, only posts relevant to AI, job loss, and employment anxiety were retained. This filtering was performed using a predefined set of keywords related to artificial intelligence, automation, layoffs, unemployment, and career anxiety, such as *AI, ChatGPT, automation, layoff, unemployment, future of work,* and *UBI*.

In total, approximately **7.8 thousand raw posts** were collected from **8 different subreddits**. After merging the data and applying the keyword-based filtering, the final dataset contains **4,837 posts**, which were saved for further analysis.


```{r load-libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(topicmodels)
library(widyr)
library(textstem)
library(fmsb)
library(spacyr)
library(dplyr)
library(tidytext)
library(stringr)
library(textstem)
library(tidyr)
library(tm)
library(ggplot2)
library(tidygraph)

library(topicmodels)
library(lubridate)

df_raw <- read.csv("data/reddit_ai_anxiety_data.csv", stringsAsFactors = FALSE)



```

## **3.Text Preprocessing and Feature Extraction**
After loading the data, basic text cleaning was applied by combining the post title and text, converting all text to lowercase, removing URLs, removing non-alphabetic characters, and trimming extra spaces. The cleaned text was stored in a new column and used as the base input for further analysis.

```{r base-cleaning, echo=TRUE, results="hide", message=FALSE, warning=FALSE}
# Initial global clean (Noise removal)
df_base_clean <- df_raw %>%
  mutate(
    text_clean = tolower(paste(title, text)),
    text_clean = str_replace_all(text_clean, "http\\S+\\s*", ""), # Remove URLs
    text_clean = str_replace_all(text_clean, "[^a-z\\s]", " "),   # Remove non-letters
    text_clean = str_squish(text_clean)                           # Remove extra spaces
  )

```

Two parallel tokenization approaches were then used. First, unigram analysis was performed by splitting the text into single words, removing stop-words and Reddit-specific noise, and applying lemmatization to normalize different word forms. Second, n-gram analysis was carried out by creating bigrams and trigrams to capture word sequences and context. Stop-words were removed from all positions in the n-grams, and additional noise terms were filtered to retain only meaningful phrases.

### a) Unigram Analysis

```{r unigram-analysis, echo=TRUE, message=FALSE, warning=FALSE}
# Define custom Reddit noise once
reddit_noise <- c("amp", "gt", "lt", "transition", "thread", "post", "edit", "don")

df_unigrams_refined <- df_base_clean %>%
  unnest_tokens(word, text_clean) %>%
  anti_join(stop_words, by = "word") %>%
  # Remove Reddit-specific noise at token level
  filter(!word %in% c("gt", "amp", "removed", "deleted")) %>%
  # Lemmatize
  mutate(lemma = lemmatize_words(word)) %>%
  # Fix known lemmatization issue
  mutate(lemma = ifelse(lemma == "datum", "data", lemma)) %>%
  # Remove additional Reddit noise at lemma level
  filter(!lemma %in% reddit_noise)

# Check top words
head(df_unigrams_refined %>% count(lemma, sort = TRUE), 10)
```


### b) N-grams Analysis

**Bigrams**
```{r ngram-analysis, echo=TRUE, message=FALSE, warning=FALSE}

# Define your custom noise list
reddit_noise <- c("amp", "gt", "lt", "transition", "thread", "post", "edit", "don")

df_bigrams_final <- df_base_clean %>%
  # 1. Create n-grams
  unnest_tokens(bigram, text_clean, token = "ngrams", n = 2) %>%
  
  # 2. Separate into columns for filtering/cleaning
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  
  # 3. Filter out standard stop words AND custom reddit noise
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word1 %in% reddit_noise,
         !word2 %in% reddit_noise) %>%
  
  # 4. Normalize words (Lemmatize and fix the "datum" issue)
  mutate(word1 = lemmatize_words(word1),
         word2 = lemmatize_words(word2),
         word1 = ifelse(word1 == "datum", "data", word1),
         word2 = ifelse(word2 == "datum", "data", word2)) %>%
  
  # 5. Recombine into the final bigram string
  unite(bigram, word1, word2, sep = " ")

# Check results
head(df_bigrams_final %>% count(bigram, sort = TRUE), 20)

```



## **4.Text Analysis Methods**

Here, we are going to use multiple text analysis methods to study the data, combining a rule-based scoring approach with topic modeling to explore new patterns in the data.

### **4.1 Rule-based Text Classification**
The main analysis applies a keyword dictionary approach based on three thematic pillars: AI and emerging technologies, job loss and employment-related stress, and anxiety or negative sentiment. For each post, keyword-based regular expressions are used to count the presence of terms from each pillar, resulting in three scores per post: ai_score, layoff_score, and anxiety_score.


```{r dictionary, echo=FALSE, message=TRUE, warning=FALSE}
# Pillar 1: AI & Emerging Tech (Adding specific models and synonyms)
ai_terms <- c("ai", "artificial", "intelligence", "gpt", "chatgpt", "llm", 
              "automation", "model", "algorithm", "generative", "bot", 
              "openai", "gemini", "claude", "-sora", "copilot", "midjourney", 
              "machine learning", "ml", "deep learning", "neural")

# Pillar 2: Employment & Economic Distress (Adding Reddit-specific job terms)
layoff_terms <- c("layoff", "fired", "redundancy", "severance", "unemployment", 
                  "outsourcing", "hiring freeze", "jobcut", "market", "replaced",
                  "jobless", "unemployed", "termination", "reskilling", "upskilling",
                  "entry level", "junior", "career", "interview", "ghosted", "application")

# Pillar 3: Anxiety & Sentiment (Adding clinical and slang terms for fear)
anxiety_terms <- c("anxious", "worried", "scared", "panic", "fear", "stress", 
                   "doom", "uncertainty", "terrified", "nervous", "depressed",
                   "anxiety", "hopeless", "dread", "fearing", "scary", "struggling", 
                   "mental health", "burnout", "overwhelmed", "suicidal")


```

To identify relevant discussions, a rule-based filtering strategy is applied. Posts mentioning at least two pillars (categories_touched ‚â• 2) are treated as broadly relevant, resulting in 701 posts, as these discussions link AI with employment concerns or emotional responses. A stricter collision rule (collision_score > 0) is then used to isolate posts in which all three pillars co-occur, resulting in 81 posts. These posts represent the strongest signal of AI-driven job insecurity, as they explicitly connect technological change, employment disruption, and emotional distress within the same discussion.


**We will calculate how many words from each list appear in every post.**
```{r keyword-scoring, echo=TRUE, message=TRUE, warning=FALSE}
# 1. Regex Patterns (Keep these the same)
ai_pattern      <- paste0("\\b(", paste(ai_terms, collapse="|"), ")\\b")
layoff_pattern  <- paste0("\\b(", paste(layoff_terms, collapse="|"), ")\\b")
anxiety_pattern <- paste0("\\b(", paste(anxiety_terms, collapse="|"), ")\\b")

# 2. Updated Scoring Engine
df_scored <- df_base_clean %>%
  mutate(
    ai_score      = str_count(text_clean, ai_pattern),
    layoff_score  = str_count(text_clean, layoff_pattern),
    anxiety_score = str_count(text_clean, anxiety_pattern),
    
    # NEW: Count how many DISTINCT categories have at least one hit
    # This turns (1, 5, 0) into 2 categories touched.
    categories_touched = (ai_score > 0) + (layoff_score > 0) + (anxiety_score > 0),
    
    # Keep your original collision score for reference
    collision_score = ai_score * layoff_score * anxiety_score
  )

# 3. Filter for 'At least 2 pillars'
# This will give you a much larger dataset for your LDA Topic Model
df_expanded_subset <- df_scored %>%
  filter(categories_touched >= 2)

# Check your new total count
print(paste("New subset size for LDA:", nrow(df_expanded_subset)))

# View the top results to ensure they look relevant
df_expanded_subset %>%
  arrange(desc(categories_touched), desc(collision_score)) %>%
  select(subreddit, ai_score, layoff_score, anxiety_score, categories_touched) %>%
  head(10)
```



```{r most-anxious-posts, echo=TRUE, message=TRUE, warning=FALSE}

df_scored %>% 
  filter(collision_score > 0) %>% 
  arrange(desc(collision_score)) %>% 
  select(subreddit, ai_score, layoff_score, anxiety_score, collision_score) %>% 
  head(10)

```

```{r pillar-overlap-summary, echo=TRUE, message=FALSE, warning=FALSE}
df_scored %>%
  summarise(
    Only_AI = sum(ai_score > 0 & layoff_score == 0 & anxiety_score == 0),
    AI_and_Layoff = sum(ai_score > 0 & layoff_score > 0),
    AI_and_Anxiety = sum(ai_score > 0 & anxiety_score > 0),
    Full_Collision = sum(collision_score > 0)
  )

```

```{r pillar, echo=TRUE, message=FALSE, warning=FALSE}


library(ggvenn)

# 2. Prepare the logical list for the diagram
# We use the full df_scored to show the 'Outer' posts as well
venn_data <- list(
  AI      = which(df_scored$ai_score > 0),
  Layoffs = which(df_scored$layoff_score > 0),
  Anxiety = which(df_scored$anxiety_score > 0)
)

# 3. Create the plot
ggvenn(venn_data, 
       fill_color = c("#0073C2FF", "#EFC000FF", "#CD534CFF"),
       stroke_size = 0.5, 
       set_name_size = 5,
       text_size = 4) +
  labs(title = "The Intersection of AI Career Dread",
       subtitle = paste("Visualizing the Pillar Collisions for the Gold Standard Subset"))


```



```{r Workhorses, echo=TRUE, message=FALSE, warning=FALSE}

# See which words are the "Workhorses" (the ones doing the most filtering)
df_unigrams_refined %>%
  filter(id %in% df_expanded_subset$id) %>%
  mutate(category = case_when(
    lemma %in% ai_terms ~ "AI",
    lemma %in% layoff_terms ~ "Layoff",
    lemma %in% anxiety_terms ~ "Anxiety",
    TRUE ~ "Other"
  )) %>%
  filter(category != "Other") %>%
  count(category, lemma, sort = TRUE) %>%
  group_by(category) %>%
  slice_max(n, n = 10)

```

### **4.2 Topic Modeling (LDA)**

To further explore the underlying structure of discussions related to AI-driven job insecurity, Latent Dirichlet Allocation (LDA) topic modeling was applied. LDA is an unsupervised text mining technique that automatically discovers latent themes in a collection of documents based on patterns of word co-occurrence. Unlike the rule-based approach, LDA does not rely on predefined keywords and instead allows themes to emerge directly from the data.
The model was applied to the subset of 701 posts identified as relevant through rule-based scoring (posts mentioning at least two thematic pillars). First, a document‚Äìterm matrix was constructed using lemmatized unigrams, where each document represents a Reddit post and each term represents a normalized word. This matrix serves as the input to the LDA model.

```{r Document-Term Matrix, echo=TRUE, message=FALSE, warning=FALSE}

# 1. Create Document-Term Matrix
# We use only the IDs from your 701-post subset
dtm_anxious <- df_unigrams_refined %>%
  filter(id %in% df_expanded_subset$id) %>%
  count(id, lemma) %>%
  cast_dtm(id, lemma, n)

# 2. Run LDA (k=5 topics)
# seed is crucial for reproducibility
lda_results <- LDA(dtm_anxious, k = 5, control = list(seed = 1234))



# 3. Get the Top 10 words per topic to see what they are
top_terms <- tidy(lda_results, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# 4. Plot the results
ggplot(top_terms, aes(beta, reorder_within(term, beta, topic), fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  labs(title = "LDA Topic Discovery", subtitle = "Top terms for the 5 hidden anxiety themes") +
  theme_minimal()
```


An LDA model with five topics (k = 5) was then estimated, with a fixed random seed to ensure reproducibility. Each topic is represented by a probability distribution over words, and each document is represented as a mixture of topics. To interpret the topics, the top terms with the highest probabilities (beta values) were extracted for each topic and visualized.

The resulting topics reveal distinct narratives within the discussions, such as concerns about AI models and automation, job roles and career progression, data and engineering skills, broader societal impacts of AI, and interview or hiring-related stress. Together, these topics provide a deeper and more exploratory view of how anxiety related to AI and job loss is discussed, complementing the earlier rule-based classification by uncovering patterns that may not be captured by predefined keyword lists.







### **4.3 Topic Assignment to Posts**

After fitting the LDA model, each post was assigned a dominant topic based on the highest topic probability. LDA represents each post as a mixture of topics, where the gamma value indicates the proportion of a topic within a post. For each post, the topic with the highest gamma value was selected as the primary topic.

This dominant topic label was then merged back into the filtered dataset, allowing posts to be analyzed at the document level. Finally, the distribution of dominant topics across subreddits was examined to understand how different communities contribute to distinct discussion themes.

```{r topic-assignment, echo=TRUE, message=FALSE, warning=FALSE}
post_topics <- tidy(lda_results, matrix = "gamma") %>%
  group_by(document) %>%
  slice_max(gamma, n = 1, with_ties = FALSE) %>%
  ungroup() %>%
  transmute(
    id = document,
    dominant_topic = topic,
    dominant_gamma = gamma
  )

df_final <- df_expanded_subset %>%
  inner_join(post_topics, by = "id")

# How confident are assignments?
summary(df_final$dominant_gamma)

df_final_conf <- df_final %>% filter(dominant_gamma >= 0.40)
table(df_final_conf$subreddit, df_final_conf$dominant_topic)
```

## **5.Results and Discussion**

### **5.1 Interpretation of LDA Topics**

The LDA model identifies five distinct themes within discussions related to AI and job insecurity. 

**Topic 1** reflects human-centered and creative concerns about AI, with strong emotional language and a clear concentration from the ArtistHate subreddit. 

**Topic 2** captures workplace-level discussions about job roles, teams, pay, and short-term employment uncertainty, mainly from cscareerquestions, jobs, and DataScience.

**Topic 3** focuses on technical careers in AI, data science, and machine learning, highlighting concerns around skills, experience, and competition in technical job markets. This topic is dominated by cscareerquestions and DataScience. 

**Topic 4** represents broader, system-level discussions about the impact of AI on workers, markets, and society, with strong contributions from Futurology and singularity. 

**Topic 5** captures immediate job-search anxiety related to interviews, hiring timelines, and pay, primarily driven by posts from jobs and antiwork.


### **5.2 Temporal Trends of LDA Topics**

To examine how discussions evolve over time, posts were grouped by month and dominant LDA topic. The monthly distribution of posts for each topic was then visualized to identify temporal patterns and shifts in topic prominence.


```{r DF_FINAL, echo=FALSE, message=FALSE, warning=FALSE}
# 1. Extract the 'Gamma' (the probability of a post belonging to a topic)
# We use 'lda_results' because that is what you named your model!
post_topics <- tidy(lda_results, matrix = "gamma") %>%
  group_by(document) %>%
  slice_max(gamma, n = 1) %>% 
  ungroup() %>%
  rename(id = document, dominant_topic = topic)

# 2. CREATE the categorized dataframe
df_final_results <- df_expanded_subset %>%
  inner_join(post_topics, by = "id")

# 3. Verify it worked
nrow(df_final_results) # Should return 701

```

```{r TREND, echo=TRUE, message=FALSE, warning=FALSE}
df_final_results$date_clean <- as.Date(as.POSIXct(df_final_results$created_utc, origin="1970-01-01"))

# 3. Create the Time-Series Trend


df_final_results %>%
  mutate(month = floor_date(date_clean, "month")) %>%
  count(month, dominant_topic) %>%
  filter(!is.na(month)) %>%  # Remove any rows where date conversion failed
  ggplot(aes(x = month, y = n, color = factor(dominant_topic))) +
  geom_line(size = 1) +
  geom_point() +
  labs(title = "Patterns and Trends",
       subtitle = "Information Diffusion of 5 themes over time",
       x = "Timeline", y = "Post Count", color = "Topic ID") +
  theme_minimal()
```
**The trend reflects the distribution of collected posts over time; later months may have higher counts because the dataset primarily contains recent **


### **5.3 Knowledge Representation**


#### **5.3.1  Subreddit‚ÄìTopic Network Analysis**
This network shows which subreddits are most strongly connected to each LDA topic, based on how many posts from each subreddit were assigned to that topic.

```{r knowledge-graph, echo=FALSE, message=FALSE, warning=FALSE}
# 1. Prepare the connections (Subreddit to Topic)
library(ggraph)
links <- df_final_results %>%
  count(subreddit, dominant_topic) %>%
  rename(from = subreddit, to = dominant_topic, weight = n) %>%
  mutate(to = paste0("Topic ", to))

# 2. Convert to a graph object
graph_data <- as_tbl_graph(links)

# 3. Plot the Social Network of Anxiety
ggraph(graph_data, layout = "fr") +
  geom_edge_link(aes(width = weight), alpha = 0.2, color = "steelblue") +
  geom_node_point(size = 5, color = "darkred") +
  geom_node_text(aes(label = name), repel = TRUE, size = 4) +
  theme_graph() +
  labs(title = "How Subreddits cluster around specific AI narratives")

```


```{r posts, echo=TRUE, message=TRUE, warning=FALSE}
df_final_results <- df_final_results %>%
  mutate(sentiment = case_when(
    collision_score > 0.7 ~ "High Anxiety",
    collision_score > 0.4 ~ "Moderate Anxiety",
    TRUE ~ "Low Anxiety"
  ))
  library(networkD3)

# Prepare Link 1: Subreddit to Topic
links_sub_topic <- df_final_results %>%
  count(subreddit, dominant_topic) %>%
  rename(source = subreddit, target = dominant_topic, value = n) %>%
  mutate(target = paste0("Topic ", target))

# Prepare Link 2: Topic to Sentiment (Now the column exists!)
links_topic_sent <- df_final_results %>%
  count(dominant_topic, sentiment) %>%
  rename(source = dominant_topic, target = sentiment, value = n) %>%
  mutate(source = paste0("Topic ", source))

# Combine links and create Nodes
links <- rbind(links_sub_topic, links_topic_sent)
nodes <- data.frame(name = unique(c(links$source, links$target)))
links$IDsource <- match(links$source, nodes$name) - 1
links$IDtarget <- match(links$target, nodes$name) - 1

# Plot the Sankey
sankeyNetwork(Links = links, Nodes = nodes, Source = "IDsource", Target = "IDtarget", 
              Value = "value", NodeID = "name", sinksRight = FALSE)


```


#### **5.3.2  Word Correlation Network**

The next step builds a concept network based on word co-occurrence within posts. First, word‚Äìword correlations were computed using unigrams from the selected subset of posts. However, using all unigrams produced a very dense and visually cluttered network, dominated by frequent and general words. To improve interpretability, the analysis was refined by restricting the network to a predefined research lexicon representing the AI, job/economic, and anxiety pillars. This refinement results in a clearer and more focused network that highlights meaningful links between the three thematic pillars.

```{r correlation, echo=FALSE, message=FALSE, warning=FALSE}
# 1. Calculate correlations between words appearing in the same post
word_cors <- df_unigrams_refined %>%
  filter(id %in% df_final_results$id) %>%
  group_by(lemma) %>%
  filter(n() >= 20) %>% # Only common words to avoid a messy graph
  pairwise_cor(lemma, id, sort = TRUE)

# 2. Filter for strong relationships and plot
word_cors %>%
  filter(correlation > .15) %>%
  as_tbl_graph() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_graph() +
  labs(title = "Word Correlation Network", subtitle = "Which concepts are linked in the collective mind?")

```
    
    
```{r relevent_corr, echo=FALSE, message=FALSE, warning=FALSE}  
# 1. Combine your existing dictionaries into one Research Lexicon
research_lexicon <- unique(c(ai_terms, layoff_terms, anxiety_terms))

# 2. Filter unigrams to include ONLY these terms from your 701 posts
df_unigrams_lexicon <- df_unigrams_refined %>%
  filter(id %in% df_final_results$id) %>%
  filter(lemma %in% research_lexicon)

# 3. Calculate correlations
word_cors_full <- df_unigrams_lexicon %>%
  pairwise_cor(lemma, id, sort = TRUE)

# 4. Define Pillar membership for coloring
# This helps the viewer see how the 3 "worlds" collide
node_metadata <- data.frame(name = research_lexicon) %>%
  mutate(pillar = case_when(
    name %in% ai_terms ~ "AI Technology",
    name %in% layoff_terms ~ "Job/Economic",
    name %in% anxiety_terms ~ "Anxiety/Emotion",
    TRUE ~ "Other"
  ))

# 5. Visualize the "Pillar Collision"
library(ggraph)
library(tidygraph)

word_cors_full %>%
  filter(correlation > 0.08) %>% # Adjusted threshold to see more 'Dual-Pillar' links
  as_tbl_graph() %>%
  left_join(node_metadata, by = "name") %>%
  ggraph(layout = "stress") + 
  geom_edge_link(aes(alpha = correlation, width = correlation), color = "gray70") +
  geom_node_point(aes(color = pillar), size = 5) +
  scale_color_manual(values = c("AI Technology" = "#0073C2FF", 
                                "Job/Economic" = "#EFC000FF", 
                                "Anxiety/Emotion" = "#CD534CFF")) + 
  geom_node_text(aes(label = name), repel = TRUE, fontface = "bold", size = 4) +
  theme_graph() +
  labs(title = "The Integrated DNA of AI Career Anxiety",
       subtitle = "Colors represent the 3 defined Pillars",
       color = "Pillar Category")
```


An interactive version of the network was also created using visNetwork. This allows users to explore relationships by dragging nodes and visually inspecting how concepts from AI, job insecurity, and anxiety are connected.

```{r INTERECTIVE, echo=FALSE, message=FALSE, warning=FALSE}

nodes <- word_cors_full %>%
  filter(correlation > 0.08) %>%
  as_tbl_graph() %>%
  activate(nodes) %>%
  left_join(node_metadata, by = "name") %>%
  as_tibble() %>%
  rename(id = name) %>%
  mutate(label = id, 
         group = pillar,
         title = paste0("Pillar: ", pillar)) # Tooltip on hover

# 2. Prepare Edges
edges <- word_cors_full %>%
  filter(correlation > 0.08) %>%
  rename(from = item1, to = item2, value = correlation)

# 3. Create Interactive Plot
library(visNetwork)

visNetwork(nodes, edges, main = "Interactive DNA of Technology and  Anxiety", 
           submain = "Drag nodes to explore ' Bridge") %>%
  visGroups(groupname = "AI Technology", color = "#0073C2FF") %>%
  visGroups(groupname = "Job/Economic", color = "#EFC000FF") %>%
  visGroups(groupname = "Anxiety/Emotion", color = "#CD534CFF") %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visPhysics(solver = "forceAtlas2Based") %>%
  visLegend()

```


## **6.Summary and Conclusion**

In this project, several text mining techniques were used to study online discussions related to AI, job security, and anxiety. By examining how people talk about these topics across different Reddit communities, the analysis aimed to better understand what drives work-related anxiety in the current context.

Based on the results, it appears that anxiety related to work is **not directly linked to AI or automation alone**. Instead, anxiety is more strongly connected to the **current job market situation**, such as mass layoffs, hiring freezes, long job searches, and uncertainty around interviews and salaries. While AI is frequently mentioned in discussions about the future of work, emotional stress seems to be triggered more by immediate economic pressures.

At the same time, AI and automation may still play an important indirect role. They can be seen as **underlying catalysts** that contribute to market instability, which then leads to job insecurity and anxiety. In this sense, AI is part of a broader process rather than the single cause of employee anxiety.

This project can be further extended by applying deeper analysis, such as studying trends over longer time periods, using more advanced sentiment or emotion detection techniques, or examining user-level behavior across discussions. Such extensions could provide a more detailed understanding of how technological change and market conditions together shape work-related anxiety.


## **7.Appendix**

### **Code and Data Availability**

All code used in this project, along with the extracted datasets and supporting scripts, is available on GitHub at the link below. The repository includes the complete data collection pipeline, text preprocessing steps, rule-based scoring methods, topic modeling (LDA), network visualizations, and interactive analyses presented in this report.
The complete project repository can be accessed at:

üîó **GitHub Repository:**  
https://github.com/Lunar-Vibes/Exploring-Anxiety-Related-to-AI-and-Job-Loss-Using-Social-Network-Mining

The repository contains:
- Scripts for Reddit data extraction using the official API  
- Extracted datasets used for analysis  
- R Markdown files used to generate the report 
- Source code for all preprocessing, analysis, and visualization steps


### **Relevant Resources**

The following resources were useful for understanding the methods and tools applied in this project:

- **Reddit API Documentation**  
  https://www.reddit.com/dev/api/  
  Official documentation for accessing Reddit data programmatically.

- **tidytext: Text Mining with R**  
  https://www.tidytextmining.com/  
  A comprehensive guide to text mining using tidy data principles in R.

- **topicmodels Package (LDA in R)**  
  https://cran.r-project.org/package=topicmodels  
  Documentation for Latent Dirichlet Allocation and related topic modeling methods.

- **Network Analysis and Visualization in R (ggraph & tidygraph)**  
  https://ggraph.data-imaginist.com/  
  https://tidygraph.data-imaginist.com/  
  Resources for creating and interpreting network-based visualizations.

- **visNetwork: Interactive Network Visualization**  
  https://datastorm-open.github.io/visNetwork/  
  Documentation for building interactive network graphs in R.




