{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420172e1-e5fd-4e42-b841-524d539f51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(httr)\n",
    "library(jsonlite)\n",
    "library(dplyr)\n",
    "\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "client_id <- \"*******************\"\n",
    "client_secret <- \"yiHZ**Nw0eQcG**************\"\n",
    "user_agent <- \"ShaheenMiningProject/1.0\"\n",
    "# Get Access Token\n",
    "token_response <- POST(\n",
    "  url = \"https://www.reddit.com/api/v1/access_token\",\n",
    "  authenticate(client_id, client_secret),\n",
    "  body = list(grant_type = \"client_credentials\"),\n",
    "  encode = \"form\",\n",
    "  add_headers(`User-Agent` = user_agent)\n",
    ")\n",
    "token <- content(token_response)$access_token\n",
    "\n",
    "# --- 2. MULTI-SUBREDDIT LOOP ---\n",
    "target_subreddits <- c(\"jobs\", \"cscareerquestions\", \"Futurology\", \"antiwork\", \n",
    "                       \"singularity\", \"technology\", \"ArtistHate\", \"DataScience\")\n",
    "\n",
    "all_data_list <- list() # To store results from all subreddits\n",
    "\n",
    "for (sub in target_subreddits) {\n",
    "  cat(\"\\n--- Starting Subreddit: r/\", sub, \" ---\\n\")\n",
    "  \n",
    "  after_id <- NULL\n",
    "  sub_posts <- list()\n",
    "  \n",
    "  # We pull 10 pages (1000 posts) per subreddit\n",
    "  for (i in 1:10) {\n",
    "    cat(\"  Fetching page\", i, \"for\", sub, \"...\\n\")\n",
    "    \n",
    "    request_url <- paste0(\"https://oauth.reddit.com/r/\", sub, \"/new.json?limit=100\")\n",
    "    if (!is.null(after_id)) {\n",
    "      request_url <- paste0(request_url, \"&after=\", after_id)\n",
    "    }\n",
    "    \n",
    "    raw_data <- GET(url = request_url, \n",
    "                    add_headers(Authorization = paste(\"bearer\", token), `User-Agent` = user_agent))\n",
    "    \n",
    "    if (status_code(raw_data) != 200) break # Stop if error\n",
    "    \n",
    "    parsed_json <- content(raw_data, \"parsed\")\n",
    "    \n",
    "    # Extract data\n",
    "    current_page_posts <- lapply(parsed_json$data$children, function(x) {\n",
    "      data.frame(\n",
    "        subreddit = sub,\n",
    "        id = x$data$id,\n",
    "        title = ifelse(is.null(x$data$title), \"\", x$data$title),\n",
    "        text = ifelse(is.null(x$data$selftext), \"\", x$data$selftext),\n",
    "        score = x$data$score,\n",
    "        created_utc = as.POSIXct(x$data$created_utc, origin=\"1970-01-01\"),\n",
    "        stringsAsFactors = FALSE\n",
    "      )\n",
    "    })\n",
    "    \n",
    "    sub_posts[[i]] <- bind_rows(current_page_posts)\n",
    "    \n",
    "    # Update pagination token\n",
    "    after_id <- parsed_json$data$after\n",
    "    if (is.null(after_id) || after_id == \"\") break\n",
    "    \n",
    "    Sys.sleep(1.5) # Be kind to the API\n",
    "  }\n",
    "  \n",
    "  all_data_list[[sub]] <- bind_rows(sub_posts)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. COMBINE AND CLEAN ---\n",
    "df_all <- bind_rows(all_data_list) %>%\n",
    "  distinct(id, .keep_all = TRUE) # Remove any cross-posts/duplicates\n",
    "\n",
    "# --- 4. ADVANCED FILTERING ---\n",
    "# Expanded keywords for AI anxiety and job displacement\n",
    "keywords <- c(\n",
    "  \"AI\", \"artificial intelligence\", \"chatgpt\", \"llm\", \"automation\",\n",
    "  \"layoff\", \"job cut\", \"replaced\", \"redundant\", \"fired\", \"unemployment\",\n",
    "  \"displacement\", \"career anxiety\", \"future of work\", \"hiring freeze\", \n",
    "  \"upskilling\", \"obsolete\", \"income\", \"UBI\"\n",
    ")\n",
    "\n",
    "pattern <- paste(keywords, collapse = \"|\")\n",
    "\n",
    "df_filtered <- df_all %>%\n",
    "  filter(grepl(pattern, paste(title, text), ignore.case = TRUE))\n",
    "\n",
    "# --- 5. SAVE ---\n",
    "write.csv(df_filtered, file = \"reddit_ai_anxiety_data.csv\", row.names = FALSE)\n",
    "\n",
    "cat(\"\\nDone!\\nTotal raw posts scraped:\", nrow(df_all), \n",
    "    \"\\nRelevant posts after filtering:\", nrow(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991f185-dafe-43f7-8dab-be4a2ba99961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c9cb8-c740-4af4-aab5-81d9e29cce3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
